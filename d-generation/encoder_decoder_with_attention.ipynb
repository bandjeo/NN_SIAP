{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "digital-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retained-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                 if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w, add_start_end=True):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    if add_start_end:\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "textile-ridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nominated-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
    "                for line in lines[:num_examples]]\n",
    "    print(word_pairs)\n",
    "\n",
    "    return zip(*word_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-professor",
   "metadata": {},
   "source": [
    "### Load recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naked-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agreed-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optimum-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/layers/layer1.json', 'r') as f:\n",
    "    dataset = json.load(f)[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collect-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/layers/layer4.json\", \"r\") as f:\n",
    "    dataset3 = json.load(f)[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gross-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record, ingredients in zip(dataset, dataset3):\n",
    "    record['ingredients'] = ' '.join(ingredients)\n",
    "    record['instructions'] = ' '.join([instruction['text'] for instruction in record['instructions']])\n",
    "\n",
    "ingredients = [record['ingredients'].lower() for record in dataset]\n",
    "instructions = [f\"{record['instructions']}\" for record in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "immediate-contest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ingredients), len(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unnecessary-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_instructions_length = 100\n",
    "length_mask = [len(instruction.split(\" \")) <= max_instructions_length for instruction in instructions]\n",
    "ingredients = [ingredient for i, ingredient in enumerate(ingredients) if length_mask[i]]\n",
    "instructions = [instruction for i, instruction in enumerate(instructions) if length_mask[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "human-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ingredients_length = 15\n",
    "length_mask = [len(ingredient.split(\" \")) <= max_ingredients_length for ingredient in ingredients]\n",
    "ingredients = [ingredient for i, ingredient in enumerate(ingredients) if length_mask[i]]\n",
    "instructions = [instruction for i, instruction in enumerate(instructions) if length_mask[i]]\n",
    "\n",
    "min_ingredients_length = 5\n",
    "length_mask = [len(ingredient.split(\" \")) >= min_ingredients_length for ingredient in ingredients]\n",
    "ingredients = [ingredient for i, ingredient in enumerate(ingredients) if length_mask[i]]\n",
    "instructions = [instruction for i, instruction in enumerate(instructions) if length_mask[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sharing-front",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8935, 8935)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ingredients), len(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adaptive-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "invisible-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(instructions, ingredients):\n",
    "    targ_lang = [preprocess_sentence(instr, True) for instr in instructions]\n",
    "    inp_lang = [preprocess_sentence(ingr, False) for ingr in ingredients]\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beautiful-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(instructions, ingredients)\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brave-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7148 7148 1787 1787\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dynamic-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(f'{t} ----> {lang.index_word[t]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "graphic-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "65 ----> sageleaf\n",
      "24 ----> carrot\n",
      "8 ----> water\n",
      "11 ----> lemon\n",
      "1011 ----> polenta\n",
      "1 ----> salt\n",
      "\n",
      "Target Language; index to word mapping\n",
      "10 ----> <start>\n",
      "1228 ----> quarter\n",
      "104 ----> lemon\n",
      "3 ----> and\n",
      "58 ----> remove\n",
      "278 ----> seeds\n",
      "1 ----> .\n",
      "514 ----> finely\n",
      "263 ----> chop\n",
      "484 ----> thyme\n",
      "320 ----> leaves\n",
      "3 ----> and\n",
      "37 ----> place\n",
      "8 ----> with\n",
      "104 ----> lemon\n",
      "5 ----> in\n",
      "40 ----> medium\n",
      "98 ----> saucepan\n",
      "1 ----> .\n",
      "12 ----> add\n",
      "247 ----> carrots\n",
      "3 ----> and\n",
      "42 ----> water\n",
      "2 ----> ,\n",
      "82 ----> bring\n",
      "6 ----> to\n",
      "7 ----> a\n",
      "61 ----> boil\n",
      "3 ----> and\n",
      "23 ----> cook\n",
      "9 ----> until\n",
      "736 ----> reduced\n",
      "6 ----> to\n",
      "202 ----> cups\n",
      "1 ----> .\n",
      "43 ----> pour\n",
      "5 ----> in\n",
      "1405 ----> polenta\n",
      "5 ----> in\n",
      "7 ----> a\n",
      "416 ----> thin\n",
      "957 ----> stream\n",
      "2 ----> ,\n",
      "750 ----> whisking\n",
      "284 ----> constantly\n",
      "2 ----> ,\n",
      "3 ----> and\n",
      "59 ----> then\n",
      "23 ----> cook\n",
      "2 ----> ,\n",
      "7020 ----> switching\n",
      "6 ----> to\n",
      "7 ----> a\n",
      "763 ----> wooden\n",
      "119 ----> spoon\n",
      "118 ----> as\n",
      "1405 ----> polenta\n",
      "630 ----> thickens\n",
      "2 ----> ,\n",
      "48 ----> about\n",
      "13 ----> minutes\n",
      "1 ----> .\n",
      "871 ----> check\n",
      "14 ----> for\n",
      "265 ----> seasoning\n",
      "3 ----> and\n",
      "31 ----> serve\n",
      "216 ----> immediately\n",
      "118 ----> as\n",
      "128 ----> an\n",
      "2617 ----> accompaniment\n",
      "6 ----> to\n",
      "664 ----> lamb\n",
      "4079 ----> shanks\n",
      "8 ----> with\n",
      "1046 ----> artichokes\n",
      "3 ----> and\n",
      "449 ----> olives\n",
      "1 ----> .\n",
      "11 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print()\n",
    "print(\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "minute-rotation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7148, 18)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "underlying-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "continued-ordering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 18]), TensorShape([64, 136]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "preliminary-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hidden-survey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 18, 256)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stretch-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "nuclear-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 18, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "oriental-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform'\n",
    "                                       )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "white-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 7058)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sixth-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reserved-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sustained-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "## @tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "protective-recommendation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.2691\n",
      "Epoch 1 Batch 100 Loss 4.4131\n",
      "Epoch 1 Loss 4.5067\n",
      "Time taken for 1 epoch 148.31 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 4.5393\n",
      "Epoch 2 Batch 100 Loss 4.3236\n",
      "Epoch 2 Loss 4.4910\n",
      "Time taken for 1 epoch 148.78 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 4.5666\n",
      "Epoch 3 Batch 100 Loss 4.1009\n",
      "Epoch 3 Loss 4.4888\n",
      "Time taken for 1 epoch 148.21 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 4.8825\n",
      "Epoch 4 Batch 100 Loss 4.5682\n",
      "Epoch 4 Loss 4.4899\n",
      "Time taken for 1 epoch 148.79 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 4.1589\n",
      "Epoch 5 Batch 100 Loss 4.4579\n",
      "Epoch 5 Loss 4.4892\n",
      "Time taken for 1 epoch 148.95 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 4.7732\n",
      "Epoch 6 Batch 100 Loss 4.3436\n",
      "Epoch 6 Loss 4.4914\n",
      "Time taken for 1 epoch 149.92 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 4.4990\n",
      "Epoch 7 Batch 100 Loss 4.3723\n",
      "Epoch 7 Loss 4.4894\n",
      "Time taken for 1 epoch 149.66 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 4.7909\n",
      "Epoch 8 Batch 100 Loss 4.6130\n",
      "Epoch 8 Loss 4.4873\n",
      "Time taken for 1 epoch 148.04 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 5.0133\n",
      "Epoch 9 Batch 100 Loss 4.5194\n",
      "Epoch 9 Loss 4.4910\n",
      "Time taken for 1 epoch 149.07 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 4.6337\n",
      "Epoch 10 Batch 100 Loss 4.4105\n",
      "Epoch 10 Loss 4.4902\n",
      "Time taken for 1 epoch 148.18 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 4.6510\n",
      "Epoch 11 Batch 100 Loss 4.4940\n",
      "Epoch 11 Loss 4.4886\n",
      "Time taken for 1 epoch 148.04 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 4.4307\n",
      "Epoch 12 Batch 100 Loss 4.3318\n",
      "Epoch 12 Loss 4.4949\n",
      "Time taken for 1 epoch 149.56 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 4.5062\n",
      "Epoch 13 Batch 100 Loss 4.1247\n",
      "Epoch 13 Loss 4.4898\n",
      "Time taken for 1 epoch 148.58 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 4.3660\n",
      "Epoch 14 Batch 100 Loss 4.7057\n",
      "Epoch 14 Loss 4.4903\n",
      "Time taken for 1 epoch 148.88 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 4.6063\n",
      "Epoch 15 Batch 100 Loss 4.3293\n",
      "Epoch 15 Loss 4.4925\n",
      "Time taken for 1 epoch 148.91 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 4.6060\n",
      "Epoch 16 Batch 100 Loss 4.3508\n",
      "Epoch 16 Loss 4.4873\n",
      "Time taken for 1 epoch 148.21 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 4.7345\n",
      "Epoch 17 Batch 100 Loss 4.2895\n",
      "Epoch 17 Loss 4.4887\n",
      "Time taken for 1 epoch 148.69 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 4.3194\n",
      "Epoch 18 Batch 100 Loss 4.6213\n",
      "Epoch 18 Loss 4.4898\n",
      "Time taken for 1 epoch 148.98 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 4.2829\n",
      "Epoch 19 Batch 100 Loss 4.2559\n",
      "Epoch 19 Loss 4.4900\n",
      "Time taken for 1 epoch 149.29 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 4.2593\n",
      "Epoch 20 Batch 100 Loss 4.7280\n",
      "Epoch 20 Loss 4.4896\n",
      "Time taken for 1 epoch 149.28 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 4.2507\n",
      "Epoch 21 Batch 100 Loss 4.5778\n",
      "Epoch 21 Loss 4.4905\n",
      "Time taken for 1 epoch 147.35 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 4.5580\n",
      "Epoch 22 Batch 100 Loss 4.4322\n",
      "Epoch 22 Loss 4.4871\n",
      "Time taken for 1 epoch 149.15 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 4.4628\n",
      "Epoch 23 Batch 100 Loss 4.7051\n",
      "Epoch 23 Loss 4.4915\n",
      "Time taken for 1 epoch 148.90 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 4.1212\n",
      "Epoch 24 Batch 100 Loss 4.6304\n",
      "Epoch 24 Loss 4.4889\n",
      "Time taken for 1 epoch 148.77 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 5.0627\n",
      "Epoch 25 Batch 100 Loss 4.6559\n",
      "Epoch 25 Loss 4.4911\n",
      "Time taken for 1 epoch 149.57 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 4.6974\n",
      "Epoch 26 Batch 100 Loss 4.9062\n",
      "Epoch 26 Loss 4.4862\n",
      "Time taken for 1 epoch 148.53 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 4.2932\n",
      "Epoch 27 Batch 100 Loss 4.2991\n",
      "Epoch 27 Loss 4.4905\n",
      "Time taken for 1 epoch 149.44 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 4.4211\n",
      "Epoch 28 Batch 100 Loss 4.6335\n",
      "Epoch 28 Loss 4.4913\n",
      "Time taken for 1 epoch 149.36 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 4.9888\n",
      "Epoch 29 Batch 100 Loss 4.0641\n",
      "Epoch 29 Loss 4.4925\n",
      "Time taken for 1 epoch 148.32 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 4.5525\n",
      "Epoch 30 Batch 100 Loss 4.5150\n",
      "Epoch 30 Loss 4.4916\n",
      "Time taken for 1 epoch 148.85 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 4.7553\n",
      "Epoch 31 Batch 100 Loss 5.0264\n",
      "Epoch 31 Loss 4.4892\n",
      "Time taken for 1 epoch 149.63 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 4.1111\n",
      "Epoch 32 Batch 100 Loss 4.2700\n",
      "Epoch 32 Loss 4.4918\n",
      "Time taken for 1 epoch 149.37 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 4.3016\n",
      "Epoch 33 Batch 100 Loss 4.5430\n",
      "Epoch 33 Loss 4.4897\n",
      "Time taken for 1 epoch 148.85 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 4.5370\n",
      "Epoch 34 Batch 100 Loss 4.1375\n",
      "Epoch 34 Loss 4.4890\n",
      "Time taken for 1 epoch 148.72 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 4.7595\n",
      "Epoch 35 Batch 100 Loss 4.4165\n",
      "Epoch 35 Loss 4.4881\n",
      "Time taken for 1 epoch 147.39 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 4.2904\n",
      "Epoch 36 Batch 100 Loss 4.5451\n",
      "Epoch 36 Loss 4.4920\n",
      "Time taken for 1 epoch 148.59 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 4.5606\n",
      "Epoch 37 Batch 100 Loss 4.4437\n",
      "Epoch 37 Loss 4.4886\n",
      "Time taken for 1 epoch 148.59 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 4.2991\n",
      "Epoch 38 Batch 100 Loss 4.4703\n",
      "Epoch 38 Loss 4.4911\n",
      "Time taken for 1 epoch 148.31 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 4.5921\n",
      "Epoch 39 Batch 100 Loss 4.1515\n",
      "Epoch 39 Loss 4.4897\n",
      "Time taken for 1 epoch 150.10 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 4.4582\n",
      "Epoch 40 Batch 100 Loss 4.4345\n",
      "Epoch 40 Loss 4.4919\n",
      "Time taken for 1 epoch 148.41 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 4.7160\n",
      "Epoch 41 Batch 100 Loss 4.7692\n",
      "Epoch 41 Loss 4.4896\n",
      "Time taken for 1 epoch 148.85 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 4.4371\n",
      "Epoch 42 Batch 100 Loss 4.3558\n",
      "Epoch 42 Loss 4.4925\n",
      "Time taken for 1 epoch 149.11 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 4.5765\n",
      "Epoch 43 Batch 100 Loss 4.6184\n",
      "Epoch 43 Loss 4.4890\n",
      "Time taken for 1 epoch 147.78 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 4.5448\n",
      "Epoch 44 Batch 100 Loss 4.2620\n",
      "Epoch 44 Loss 4.4896\n",
      "Time taken for 1 epoch 149.04 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 4.4393\n",
      "Epoch 45 Batch 100 Loss 4.2704\n",
      "Epoch 45 Loss 4.4911\n",
      "Time taken for 1 epoch 148.73 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 4.3241\n",
      "Epoch 46 Batch 100 Loss 4.4720\n",
      "Epoch 46 Loss 4.4901\n",
      "Time taken for 1 epoch 148.89 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 4.6986\n",
      "Epoch 47 Batch 100 Loss 5.0101\n",
      "Epoch 47 Loss 4.4899\n",
      "Time taken for 1 epoch 149.77 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 4.1392\n",
      "Epoch 48 Batch 100 Loss 4.4513\n",
      "Epoch 48 Loss 4.4878\n",
      "Time taken for 1 epoch 147.82 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 4.3052\n",
      "Epoch 49 Batch 100 Loss 4.6450\n",
      "Epoch 49 Loss 4.4921\n",
      "Time taken for 1 epoch 149.89 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 4.6590\n",
      "Epoch 50 Batch 100 Loss 4.5922\n",
      "Epoch 50 Loss 4.4924\n",
      "Time taken for 1 epoch 149.88 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 4.5646\n",
      "Epoch 51 Batch 100 Loss 4.4640\n",
      "Epoch 51 Loss 4.4868\n",
      "Time taken for 1 epoch 148.33 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 4.6553\n",
      "Epoch 52 Batch 100 Loss 4.4013\n",
      "Epoch 52 Loss 4.4914\n",
      "Time taken for 1 epoch 148.91 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 3.9102\n",
      "Epoch 53 Batch 100 Loss 4.6286\n",
      "Epoch 53 Loss 4.4904\n",
      "Time taken for 1 epoch 147.81 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 4.6086\n",
      "Epoch 54 Batch 100 Loss 4.1744\n",
      "Epoch 54 Loss 4.4918\n",
      "Time taken for 1 epoch 148.15 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 4.4590\n",
      "Epoch 55 Batch 100 Loss 4.7061\n",
      "Epoch 55 Loss 4.4935\n",
      "Time taken for 1 epoch 149.28 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 4.4236\n",
      "Epoch 56 Batch 100 Loss 4.2062\n",
      "Epoch 56 Loss 4.4903\n",
      "Time taken for 1 epoch 148.34 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 4.8748\n",
      "Epoch 57 Batch 100 Loss 4.7310\n",
      "Epoch 57 Loss 4.4883\n",
      "Time taken for 1 epoch 149.53 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 4.5729\n",
      "Epoch 58 Batch 100 Loss 4.6244\n",
      "Epoch 58 Loss 4.4898\n",
      "Time taken for 1 epoch 148.62 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 4.4748\n",
      "Epoch 59 Batch 100 Loss 4.4242\n",
      "Epoch 59 Loss 4.4890\n",
      "Time taken for 1 epoch 147.82 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 4.6329\n",
      "Epoch 60 Batch 100 Loss 4.9396\n",
      "Epoch 60 Loss 4.4888\n",
      "Time taken for 1 epoch 149.57 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 4.4569\n",
      "Epoch 61 Batch 100 Loss 4.5692\n",
      "Epoch 61 Loss 4.4913\n",
      "Time taken for 1 epoch 149.20 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 4.3378\n",
      "Epoch 62 Batch 100 Loss 4.5779\n",
      "Epoch 62 Loss 4.4914\n",
      "Time taken for 1 epoch 148.73 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 4.6783\n",
      "Epoch 63 Batch 100 Loss 4.2655\n",
      "Epoch 63 Loss 4.4874\n",
      "Time taken for 1 epoch 149.29 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 4.0607\n",
      "Epoch 64 Batch 100 Loss 4.6976\n",
      "Epoch 64 Loss 4.4916\n",
      "Time taken for 1 epoch 147.99 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 4.3099\n",
      "Epoch 65 Batch 100 Loss 4.7707\n",
      "Epoch 65 Loss 4.4891\n",
      "Time taken for 1 epoch 149.44 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 4.8409\n",
      "Epoch 66 Batch 100 Loss 4.4003\n",
      "Epoch 66 Loss 4.4897\n",
      "Time taken for 1 epoch 148.65 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 4.5447\n",
      "Epoch 67 Batch 100 Loss 4.5480\n",
      "Epoch 67 Loss 4.4908\n",
      "Time taken for 1 epoch 148.61 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 4.4045\n",
      "Epoch 68 Batch 100 Loss 4.7872\n",
      "Epoch 68 Loss 4.4930\n",
      "Time taken for 1 epoch 148.77 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 4.3241\n",
      "Epoch 69 Batch 100 Loss 4.4749\n",
      "Epoch 69 Loss 4.4880\n",
      "Time taken for 1 epoch 148.06 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 4.3062\n",
      "Epoch 70 Batch 100 Loss 4.2831\n",
      "Epoch 70 Loss 4.4876\n",
      "Time taken for 1 epoch 149.97 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 4.2922\n",
      "Epoch 71 Batch 100 Loss 5.0241\n",
      "Epoch 71 Loss 4.4911\n",
      "Time taken for 1 epoch 148.90 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 0 Loss 4.4036\n",
      "Epoch 72 Batch 100 Loss 4.5947\n",
      "Epoch 72 Loss 4.4898\n",
      "Time taken for 1 epoch 148.93 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 4.4734\n",
      "Epoch 73 Batch 100 Loss 4.1766\n",
      "Epoch 73 Loss 4.4881\n",
      "Time taken for 1 epoch 148.80 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 4.4676\n",
      "Epoch 74 Batch 100 Loss 4.7054\n",
      "Epoch 74 Loss 4.4866\n",
      "Time taken for 1 epoch 148.57 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 4.2658\n",
      "Epoch 75 Batch 100 Loss 4.4012\n",
      "Epoch 75 Loss 4.4887\n",
      "Time taken for 1 epoch 148.99 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 4.8480\n",
      "Epoch 76 Batch 100 Loss 4.2038\n",
      "Epoch 76 Loss 4.4878\n",
      "Time taken for 1 epoch 148.74 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 4.2871\n",
      "Epoch 77 Batch 100 Loss 4.2921\n",
      "Epoch 77 Loss 4.4915\n",
      "Time taken for 1 epoch 148.68 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 4.3695\n",
      "Epoch 78 Batch 100 Loss 4.5401\n",
      "Epoch 78 Loss 4.4911\n",
      "Time taken for 1 epoch 149.01 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 4.3524\n",
      "Epoch 79 Batch 100 Loss 4.5788\n",
      "Epoch 79 Loss 4.4885\n",
      "Time taken for 1 epoch 149.40 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 4.9015\n",
      "Epoch 80 Batch 100 Loss 4.3413\n",
      "Epoch 80 Loss 4.4890\n",
      "Time taken for 1 epoch 149.27 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 4.6170\n",
      "Epoch 81 Batch 100 Loss 4.6277\n",
      "Epoch 81 Loss 4.4909\n",
      "Time taken for 1 epoch 148.51 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 4.5293\n",
      "Epoch 82 Batch 100 Loss 4.7231\n",
      "Epoch 82 Loss 4.4906\n",
      "Time taken for 1 epoch 148.81 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 4.2022\n",
      "Epoch 83 Batch 100 Loss 4.6663\n",
      "Epoch 83 Loss 4.4915\n",
      "Time taken for 1 epoch 149.40 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 4.5838\n",
      "Epoch 84 Batch 100 Loss 4.1467\n",
      "Epoch 84 Loss 4.4862\n",
      "Time taken for 1 epoch 149.16 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 4.7137\n",
      "Epoch 85 Batch 100 Loss 4.6416\n",
      "Epoch 85 Loss 4.4907\n",
      "Time taken for 1 epoch 148.89 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 4.6869\n",
      "Epoch 86 Batch 100 Loss 4.3833\n",
      "Epoch 86 Loss 4.4915\n",
      "Time taken for 1 epoch 149.62 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 4.6138\n",
      "Epoch 87 Batch 100 Loss 4.7303\n",
      "Epoch 87 Loss 4.4906\n",
      "Time taken for 1 epoch 148.41 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 4.8458\n",
      "Epoch 88 Batch 100 Loss 4.5625\n",
      "Epoch 88 Loss 4.4911\n",
      "Time taken for 1 epoch 149.58 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 4.5125\n",
      "Epoch 89 Batch 100 Loss 4.7295\n",
      "Epoch 89 Loss 4.4915\n",
      "Time taken for 1 epoch 149.25 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 4.0779\n",
      "Epoch 90 Batch 100 Loss 4.6355\n",
      "Epoch 90 Loss 4.4904\n",
      "Time taken for 1 epoch 148.66 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 4.6585\n",
      "Epoch 91 Batch 100 Loss 4.5106\n",
      "Epoch 91 Loss 4.4918\n",
      "Time taken for 1 epoch 148.40 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 4.3770\n",
      "Epoch 92 Batch 100 Loss 4.6334\n",
      "Epoch 92 Loss 4.4898\n",
      "Time taken for 1 epoch 149.76 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 4.4116\n",
      "Epoch 93 Batch 100 Loss 4.3398\n",
      "Epoch 93 Loss 4.4916\n",
      "Time taken for 1 epoch 148.03 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 4.2623\n",
      "Epoch 94 Batch 100 Loss 4.3098\n",
      "Epoch 94 Loss 4.4906\n",
      "Time taken for 1 epoch 149.35 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 4.5682\n",
      "Epoch 95 Batch 100 Loss 4.4884\n",
      "Epoch 95 Loss 4.4901\n",
      "Time taken for 1 epoch 149.17 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 4.6304\n",
      "Epoch 96 Batch 100 Loss 4.1858\n",
      "Epoch 96 Loss 4.4910\n",
      "Time taken for 1 epoch 147.85 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 4.3458\n",
      "Epoch 97 Batch 100 Loss 4.5052\n",
      "Epoch 97 Loss 4.4894\n",
      "Time taken for 1 epoch 149.55 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 4.7320\n",
      "Epoch 98 Batch 100 Loss 4.6630\n",
      "Epoch 98 Loss 4.4897\n",
      "Time taken for 1 epoch 148.12 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 4.7146\n",
      "Epoch 99 Batch 100 Loss 4.7096\n",
      "Epoch 99 Loss 4.4881\n",
      "Time taken for 1 epoch 148.02 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 4.4708\n",
      "Epoch 100 Batch 100 Loss 4.8124\n",
      "Epoch 100 Loss 4.4906\n",
      "Time taken for 1 epoch 149.32 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "weekly-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "alternate-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence, False)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = sample(predictions[0])\n",
    "\n",
    "#         predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hazardous-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "negative-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, verbose=False):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    if verbose:\n",
    "        print('Input:', sentence)\n",
    "        print('Predicted translation:', result)\n",
    "\n",
    "        attention_plot = attention_plot[:len(result.split(' ')),\n",
    "                                      :len(sentence.split(' '))]\n",
    "        plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cultural-study",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt-50'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "recognized-quality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd42122adf0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "careful-dragon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cook macaroni according to package directions; drain well. Cold. Combine macaroni, cheese cubes, celery, green pepper and pimento. Blend together mayonnaise or possibly salad dressing, vinegar, salt and dill weed; add in to macaroni mix. Toss lightly. Cover and refrigeratewell. Serve salad in lettuce lined bowl if you like. Makes 6 servings.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "polish-nevada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.20653614e-11 9.99999830e-01 2.73159635e-08 ... 1.48696360e-11\n",
      " 1.20092492e-11 1.23511076e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n",
      "[1.20325448e-11 9.99999830e-01 2.72718690e-08 ... 1.48286531e-11\n",
      " 1.19774526e-11 1.23187333e-11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(ingredients[102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-flour",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "animated-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 1000\n",
    "input_tensors = input_tensor_val[:num_examples]\n",
    "target_tensors = target_tensor_val[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "functioning-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(tensors, lang):\n",
    "    texts = []\n",
    "    for tensor in tensors:\n",
    "        text = ''\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                text += lang.index_word[t] + ' '\n",
    "        texts.append(text[:-1])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accepting-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strings = convert_to_string(input_tensors, inp_lang)\n",
    "target_strings = convert_to_string(target_tensors, targ_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "distinguished-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:47<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "generated_texts = []\n",
    "for ingrs in tqdm(input_strings):\n",
    "    generated_texts.append(translate(ingrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "architectural-making",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ',\n",
       " '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ',\n",
       " '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ',\n",
       " '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ',\n",
       " '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-template",
   "metadata": {},
   "source": [
    "#### Using rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_text_for_rouge(samples):\n",
    "    samples = [text.translate(str.maketrans('', '', string.punctuation)) for text in samples]\n",
    "    samples = [text.lower() for text in samples]\n",
    "    samples = [' '.join(text.split()) for text in samples]\n",
    "#     samples = [text.split() for text in samples]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(generated_texts)):\n",
    "    generated_texts[i] = 'put sour pancake to boil for 15 to 20 minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_rouge = preprocess_text_for_rouge(generated_texts)\n",
    "target_text_rouge = preprocess_text_for_rouge(target_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_rouge[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "\n",
    "rouge = rouge.Rouge()\n",
    "rouge.get_scores(generated_text_rouge, target_text_rouge, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-spider",
   "metadata": {},
   "source": [
    "#### Using bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_generated_text_for_bleu(samples):\n",
    "    samples = [text.translate(str.maketrans('', '', string.punctuation)) for text in samples]\n",
    "    samples = [text.lower() for text in samples]\n",
    "    samples = [' '.join(text.split()) for text in samples]\n",
    "    samples = [text.split() for text in samples]\n",
    "    return samples\n",
    "\n",
    "import string\n",
    "def preprocess_reference_text_for_bleu(samples):\n",
    "    samples = [text.translate(str.maketrans('', '', string.punctuation)) for text in samples]\n",
    "    samples = [text.lower() for text in samples]\n",
    "    samples = [' '.join(text.split()) for text in samples]\n",
    "    samples = [[text.split()] for text in samples]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_bleu = preprocess_generated_text_for_bleu(generated_texts)\n",
    "target_text_bleu = preprocess_reference_text_for_bleu(target_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "score_1_gram = corpus_bleu(target_text_bleu, generated_text_bleu, weights=(1, 0, 0, 0))\n",
    "score_2_gram = corpus_bleu(target_text_bleu, generated_text_bleu, weights=(0, 1, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1_gram, score_2_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
