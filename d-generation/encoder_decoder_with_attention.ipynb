{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "digital-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "retained-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                 if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "textile-ridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nominated-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
    "                for line in lines[:num_examples]]\n",
    "    print(word_pairs)\n",
    "\n",
    "    return zip(*word_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-professor",
   "metadata": {},
   "source": [
    "### Load recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "naked-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "agreed-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "optimum-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/layers/layer1_mini.json', 'r') as f:\n",
    "    dataset = json.load(f)[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "collect-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/layers/layer4_mini.json\", \"r\") as f:\n",
    "        dataset3 = json.load(f)[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "gross-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record, ingredients in zip(dataset, dataset3):\n",
    "    record['ingredients'] = ' '.join(ingredients)\n",
    "    record['instructions'] = ' '.join([instruction['text'] for instruction in record['instructions']])\n",
    "\n",
    "ingredients = [record['ingredients'].lower() for record in dataset]\n",
    "instructions = [f\"\\t{record['instructions']}\\n\" for record in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unnecessary-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_instructions_length = 100\n",
    "length_mask = [len(instruction.split(\" \")) <= max_instructions_length for instruction in instructions]\n",
    "ingredients = [ingredient for i, ingredient in enumerate(ingredients) if length_mask[i]]\n",
    "instructions = [instruction for i, instruction in enumerate(instructions) if length_mask[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "human-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ingredients_length = 15\n",
    "length_mask = [len(ingredient.split(\" \")) <= max_ingredients_length for ingredient in ingredients]\n",
    "ingredients = [ingredient for i, ingredient in enumerate(ingredients) if length_mask[i]]\n",
    "instructions = [instruction for i, instruction in enumerate(instructions) if length_mask[i]]\n",
    "\n",
    "min_ingredients_length = 5\n",
    "length_mask = [len(ingredient.split(\" \")) >= min_ingredients_length for ingredient in ingredients]\n",
    "ingredients = [ingredient for i, ingredient in enumerate(ingredients) if length_mask[i]]\n",
    "instructions = [instruction for i, instruction in enumerate(instructions) if length_mask[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adaptive-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "invisible-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(instructions, ingredients):\n",
    "    targ_lang = [preprocess_sentence(instr) for instr in instructions]\n",
    "    inp_lang = [preprocess_sentence(ingr) for ingr in ingredients]\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beautiful-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(instructions, ingredients)\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "brave-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574 3574 894 894\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dynamic-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(f'{t} ----> {lang.index_word[t]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "graphic-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "350 ----> waterchestnut\n",
      "4 ----> butter\n",
      "180 ----> kraftgratedparmesancheese\n",
      "162 ----> cracker\n",
      "265 ----> vegetables\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "9 ----> <start>\n",
      "16 ----> heat\n",
      "34 ----> oven\n",
      "6 ----> to\n",
      "55 ----> degrees\n",
      "67 ----> f\n",
      "1 ----> .\n",
      "19 ----> mix\n",
      "648 ----> cracker\n",
      "222 ----> crumbs\n",
      "3 ----> and\n",
      "35 ----> butter\n",
      "11 ----> until\n",
      "161 ----> blended\n",
      "1 ----> .\n",
      "29 ----> combine\n",
      "65 ----> remaining\n",
      "26 ----> ingredients\n",
      "5 ----> in\n",
      "700 ----> qt\n",
      "1 ----> .\n",
      "214 ----> casserole\n",
      "46 ----> top\n",
      "8 ----> with\n",
      "814 ----> crumb\n",
      "25 ----> mixture\n",
      "1 ----> .\n",
      "28 ----> bake\n",
      "78 ----> min\n",
      "1 ----> .\n",
      "15 ----> or\n",
      "11 ----> until\n",
      "263 ----> heated\n",
      "132 ----> through\n",
      "1 ----> .\n",
      "10 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print()\n",
    "print(\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minute-rotation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3574, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "underlying-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "continued-ordering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 20]), TensorShape([64, 136]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "preliminary-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hidden-survey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 20, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stretch-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nuclear-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "oriental-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "white-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 5255)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sixth-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "reserved-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sustained-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "## @tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "protective-recommendation",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b63ea3621a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-d3eb1b01b2ac>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(inp, targ, enc_hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# passing enc_output to the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-779ec6346135>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# passing the concatenated vector to the GRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# output shape == (batch_size * 1, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_RUNTIME_UNKNOWN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       last_output, outputs, runtime, states = self._defun_gru_call(\n\u001b[0m\u001b[1;32m    471\u001b[0m           inputs, initial_state, training, mask, row_lengths)\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36m_defun_gru_call\u001b[0;34m(self, inputs, initial_state, training, mask, sequence_lengths)\u001b[0m\n\u001b[1;32m    541\u001b[0m           \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpu_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgpu_gru_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m           last_output, outputs, new_h, runtime = standard_gru(\n\u001b[0m\u001b[1;32m    544\u001b[0m               **normal_gru_kwargs)\n\u001b[1;32m    545\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_gru\u001b[0;34m(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m   last_output, outputs, new_states = K.rnn(\n\u001b[0m\u001b[1;32m    622\u001b[0m       \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4493\u001b[0;31m       final_outputs = control_flow_ops.while_loop(\n\u001b[0m\u001b[1;32m   4494\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4495\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2733\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2734\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   4477\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4478\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4479\u001b[0;31m         output, new_states = step_function(current_input,\n\u001b[0m\u001b[1;32m   4480\u001b[0m                                            tuple(states) + tuple(constants))\n\u001b[1;32m   4481\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;31m# hidden state projected by all gate matrices at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0mmatrix_inner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0mmatrix_inner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   3312\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3313\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3314\u001b[0;31m       return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3315\u001b[0m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/recipes/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5525\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5527\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5528\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5529\u001b[0m         transpose_b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "alternate-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hazardous-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "negative-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input:', sentence)\n",
    "    print('Predicted translation:', result)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')),\n",
    "                                  :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "recognized-quality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3379ae9430>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "polish-nevada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'water mayonnaise greenonion celery tacochips'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "broadband-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> salt salt salt salt <end>\n",
      "Predicted translation: in a large bowl , and pepper . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-664a0a1097ae>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-38-664a0a1097ae>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAJwCAYAAAB1Qz2LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiPElEQVR4nO3de5hkdX3n8fcHhkEBQUURRBE1QVAELyNg1DiGJBJB99nEaFARNMuwGiPGVRNjiGa9rUqiRM0quYgIxhCNIXgjEBhBBQmarCISvICIgIKA3IcZ5rt/nBop22Hm1zNddbpPv1/PMw9ddU5XfX/DPPXuU6e6KlWFJEkttuh7AEnSwmE0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYzGlCT5xSRnJXls37NI0qYyGtNzOLAceGnPc0jSJotvWDh5SQJcDpwBPBt4cFXd1etQkrQJPNKYjuXAfYBXAmuAZ/U6jSRtIqMxHYcDH6+q24CPjS5L0oLj01MTlmRb4Grg4Ko6N8njgPOAXarqxj5nk6TZ8khj8n4LuK6qzgWoqv8EvgX8Tp9DSVo4kmyb5MVJduh7FqMxeYcBJ8247iTgiOmPImmBeh7wIbrHk1759NQEJXkocBmwV1V9a+z6h9C9murRVXVpT+NJWiCSnA08CLitqpb1OovRkKT5K8nuwKXAfsD5wBOq6uK+5vHpqQlLstvo9zTWu23a80hacA4Dzh2dD/0MPb/60mhM3mXAA2demWTH0TZJ2pAXAx8ZfX0y8MJ7+kF0GozG5AVY33OA2wF3THkWSQtIkl8CdgE+PrrqNGAb4Ff7mmlJX3c8dEn+cvRlAW9PctvY5i3pnp/8z2nPJWlBORw4tapuAaiqO5OcQvfqyzP6GMhoTM66d7MNsBdw59i2O4GvAsdOeyhJC0OSreleanvojE0nAacn2W5dTKY6l6+empzR846nAC+tqpv7nkfSwpHkAXTvU3dSVa2dse1FwJlVdc3U5zIak5NkS7rzFvv2+RI5SZorngifoNHbn38PWNr3LJI0FzzSmLAkh9M9J/miqrqu73kkzW9JLmP9r7j8OVX1iAmP83M8ET55rwEeDvwgyZXAreMbq2qfXqaSNF+9b+zr7YBXAxfQvTs2wJPpXn3551OeCzAa0/Dxje8iSZ2q+mkMkpwAvKOq3ja+T5LXA4+Z8mjdffv0lCTNT0luonuvqW/PuP4XgK9W1fbTnskT4ZI0f91K93HRMy0HblvP9RPn01MTlmQp8Aa6k+G7AVuNb6+qLfuYS9KC8G7g/UmW0b3DLcABdL8p/qY+BjIak/dm4PnA2+n+AbwW2J3uk/uO6W8sSfNdVb0zyeXA0XS/HQ7wTeDwqjqlj5k8pzFho5fPvayqPpfkZuBxVfWdJC8DDqyq5/Y8oiQ180hj8h4ErPtt8FuA+46+/hzwjj4GkrTwJLkvM85DV9X1057DE+GTdwXw4NHX3waeOfr6ycDtvUwkaUFI8rAkn01yO/Bj4NrRn+tG/506jzQm75PAgXQnsY4D/j7JkcCuwLv6HGxTjT5x8Ps147nN0Rs0PrSqruhnss031LUNdV0w7LUBH6J7duJ3gato/E3xSfKcxpQl2R94CnBpVX2q73k2RZK7gF2q6kczrt8R+NFCfkXYUNc21HXB4Nd2C3BAVV3U9yzreKQxYUl+GfhSVa0BqKovA19OsiTJL1fVOf1OuEmG/GmEQ13bUNcFw17bZcDWfQ8xzmhM3tl0H9f4oxnX7zDatmB+ChrypxEOdW1DXRcMe21jjqZb28tn/lZ4X4zG5N3TT0E7MuPNCxeAIX8a4VDXNtR1wbDXts6pdEca/5VkFbBmfGMfbyPiOY0JSfIvoy8PBs4EVo1t3hLYG/hmVR007dk2V5IPAUdX1U19zzLXhrq2oa4LBr+2wze0vao+PK1Z1jEaEzL6hwzdr/ufws++vPZO4HLgr/2MDUkLidGYsCRvBI6tqoX2VNTPGDty2qiqes4kZ5lrQ13bUNcFw17bTEkeBBwGPBI4pqquS/IU4Kqqumza83hOY/LePH4hyc7AIcDFVfWlfkbaJD/ue4AJGurahrouGPbafirJE4F/o3sV1WPofrfrOuDXgD2AF0x9Jo80JivJZ4HPVdVxSbYDLgG2pXs54O9W1Ym9Dihp3kpyNnBOVb1x9N51+1bVd5M8GfhYVT1s2jP5NiKTtww4a/T1bwI3ATsBR9J9FKwk3ZMnAus72X013fvaTZ1PT03edsCNo69/HfhkVa1Ochbw/t6m2kxJnsHdnxGydHxbVf1KL0PNkaGubajrgkGv7Xbgfuu5fk9+/ne/psIjjcm7AnhKkm3p3qzwjNH196enT97aXEmOAD4L3IfuE8SupfuH/QTufkffBWmoaxvqumDYa6P7PY03Jln3W+GVZHe6d8j+RB8DGY3J+wvgI8CVwA+AdW8b8svA1/saajO9BnhFVR0KrAZeX1WPB06ie/v3hWyoaxvqumD4a7s/XQi3Ab5A927ZPwH+pI+BPBE+BaNXQOwGnFFVt4yuOxi4saq+2Otwm2D0dg2PrqrLk1wH/EpVfS3JnsDKqtq55xE32VDXNtR1wbDXtk6SX6E7ctoC+GpVndnXLJ7TmKAkOwD7VNW5wFdmbL6RhXvo/GO6pwKgO3raG/ga3Vuj3LuvoebIUNc21HXBQNc2/vhRVWdx9wtqGP2exsVVdcO05/LpqclaC3x29D/4p5LsS/cPYMG8WeEM59Kd1Ifut93/cvQb8H/P3edsFqqhrm2o64Lhrm1ePn749NSEJTkZuKWqjhq77lhgj4X6m6pJ7g/cq6quSrIF8FpGnxECvKWqbuxzvs0x1LUNdV0w+LXNu8cPjzQm70Tgt5MsBRj9o34BcEKfQ22mnRk9HVBVa+neTfRm4PrRfxeyoa5tqOuCYa9t3j1+GI3JO4PutdaHjC4fSPc68tN6m2jz/R3weIAkDwX+me4lji8H3tLfWHNiqGsb6rpg2Gubd48fRmPCRj/5nAS8eHTVYcA/VNXq/qbabHvS/TQH8Fzggqp6Ft3aDu1tqrkx1LUNdV0w4LXNx8cPXz01HScCX0myG/Df6X5aWMi25O4PvDkQ+Mzo6+/Q01sbzKGhrm2o64Jhrw3m2eOHRxpTUFXfAC4CTgaurKoLeh5pc10EvCzJ0+j+AX9udP2udO/AuZANdW1DXRcMe23z7vHDaEzPiXSv6BjCu9r+Id0bLq4E/r6q1v1m+3OAhR7Eoa5tqOuCYa9tnXnz+OFLbqdk9LLA3wc+WFXX9D3P5kqyJbD9+C8Xjd4T57aq6uWN1ObKUNc21HXBsNcG8+vxw2hIkpr59JQkqZnRkCQ1MxpTlGRF3zNMylDX5roWnqGubb6sy2hM17z4nz4hQ12b61p4hrq2ebEuoyFJarboXz21NFvXvdh2Kve1mlVsxdYb33EBGuraXNfCM9W1ZTp3A7C6VrFVprOuO+pW7qxV613don8bkXuxLftnob+rh6Q+ZMkwH0LPX3P6PW7z6SlJUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKnZgo1GkhOSfKrvOSRpMVnS9wCb4WggfQ8hSYvJgo1GVf2k7xkkabFZsNFIcgLwgKo6JMlK4GLgRmAFsBY4EXhdVa3ta0ZJGpoFe05jPV4IrAF+CXgF8Crg+X0OJElDM6RoXFxVf1pVl1bVKcDZwIHr2zHJiiQXJrlwNaumO6UkLWBDisbXZly+CthpfTtW1fFVtayqlm3F1pOfTJIGYkjRWD3jcjGs9UlS73xQlSQ1MxqSpGZGQ5LUbMH+nkZVHTH29fINbZckzQ2PNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1W7CfET5XfnGfW/nM577a9xhz7ourhvvzwDuefkjfI0zEmit/0PcImqW6666+R5iMuudNw31kkSTNOaMhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSs8FEI8lBSc5NckOS65OcnmSvvueSpCEZTDSAbYH3APsBy4GfAKclWdrjTJI0KEv6HmCuVNUnxi8neQlwE11EvjBj2wpgBcBuuw7mr0CSJm4wRxpJHpnko0m+k+Qm4Id069tt5r5VdXxVLauqZQ/YcTB/BZI0cUP6MftTwJXAUcAPgDXAxYBPT0nSHBlENJLsCOwJvLyqzh5d9wQGsj5Jmi+G8qB6A3AdcGSS7wO7Au+iO9qQJM2RQTyhX1VrgecD+wAXAe8HjgFW9TmXJA3NUI40qKqzgL1nXL1dH7NI0lAN4khDkjQdRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSsyV9D9C3S26/P0/92m/3Pcac2/N+P+p7hIm57N337XuEiXjo867ue4TJWXtX3xNojnikIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1Gyi0UhyQpJPTfI+JEnT45GGJKnZvI5Gkq36nkGSdLepRSPJQUnOTXJDkuuTnJ5kr7HtuyepJIcmOSvJ7cBRSZYkeffo+24Yff1/k6wc+94keV2S7yS5PcnXk7xoWmuTpMVimkca2wLvAfYDlgM/AU5LsnTGfm8H/gp4NPDPwGuAI4D/ARxAN/MLZnzPW4DfBX5v9H1vBz6Y5OA5X4UkLWJLpnVHVfWJ8ctJXgLcRBeRL4xtem9VfXxsv6OBd6z7/iSvAg4a274t8Grg16vq3NHVlyXZjy4in545S5IVwAqApTttv9lrk6TFYmrRSPJI4M3A/sAD6Y4YtgB2m7HrhWPfswOwM3DBuuuqqpJcADx0dNWjgXsBn0tSY7ezFXD5+mapquOB4wG23WOXWt8+kqSfN7VoAJ8CrgSOAn4ArAEuBmY+PXXrLG933VNszwaumLFt9SxvS5K0AVOJRpIdgT2Bl1fV2aPrnrCx+6+qnyS5BngScNbo+zK6fM1ot4uBVcDDquqsyaxAkgTTO9K4AbgOODLJ94FdgXfRHW1szHHA65JcSheIo4BdgKsBqurmJMcCx46Ccg6wHd1J87Wjp6IkSXNgKq+eqqq1wPOBfYCLgPcDx9AdIWzMscBHgA8B54+u+yRwx9g+xwBvonul1TeAM4DfAi7b/OklSetM9Eijqo4Y+/osYO8Zu2w3tv1yIOu5jTXAq0Z/AEjyH4y94qqqCnjv6I8kaUKmeSJ8kyR5GPBM4PN0r4g6ku6I5cg+55KkxWjeRwNYC7yY7hzIFnTnNX6jqi7c4HdJkubcvI9GVX0feGrfc0iS5vkbFkqS5hejIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkprN+w9hmrQts5btl67qe4w5twXV9wgTs3btz32U/DDU2r4nkDbKIw1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmU4lGkpVJ3jeN++rj/iRpsfBIQ5LUzGhIkppNMxpLkhyX5IbRn3cl2QIgyf2SfHh0/e1JzkzymHXfmOTqJL8zdvkLSW5OsmR0+ReSVJKHTHE9krToTDMaLxzd35OBo4AVwKtG204A9gf+G7AfcBvwuST3Hm3/PLAcIMk2wJOAVcCy0fblwHeq6srJLkGSFrclU7yvq4FXVlUBlyTZA3h1ktOA5wBPr6pzAJIcBlxBF5q/AVYCfzC6nV8Cvgt8GXgGcD5dNFa2DpJkBV20uNeD7rOZy5KkxWOaRxrnj4KxznnArsBewNrRZQCq6ifA14FHj65aCeyRZBe6QJw9um75aPvTmUU0qur4qlpWVcu22uHeG/8GSRIw/0+EF0BVXQJcQ3dksZy7o/GUJHsBD2EW0ZAkbZppRmP/JBm7fABwFfBN7j7XAUCS7YHHAheP7f954GC68xgrq+py4DrgdXg+Q5KmYprReDDwniSPSvJc4LXAu6vqW8CpwAeTPC3JY4GTgJuAj459/0rgecC3q+rasetehEcZkjQV04zGycCWdCew/xr4W+Ddo20vAS4A/mX0322Ag6rq9rHvX0l34n7lRq6TJE3IVF49VVXLxy6+Yj3bbwAO38htXAJkxnUn0L1cd0P3J0maI/P9RLgkaR4xGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1GwqnxE+nxXhzrVb9j2GZmHt2mx8pwVoi6237nuEiVl7xx19j6A54pGGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmizIaSVYkuTDJhatvvK3vcSRpwViU0aiq46tqWVUt2+q+2/Q9jiQtGIsyGpKkTWM0JEnNjIYkqZnRkCQ1G2w0khyRpJLs3vcskjQUg40G8HDgYuDKvgeRpKEYcjSeBfxeVa3pexBJGoolfQ8wKVX1pL5nkKShGfKRhiRpjhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaDfaT+1ptmbXcZ+mqvseYc6truD8PrPnhNn2PMBlbDPf/GUnfE2g26p43DfhfqSRprhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUbbDSSLEtSSXbvexZJGorBRkOSNPeMhiSp2byIRpKDkpyb5IYk1yc5Pcleo227j55m+q0kZyS5LcnFSX5tPbdxSZI7kpwL7NHLYiRpwOZFNIBtgfcA+wHLgZ8ApyVZOrbPW4G/BPYF/h34WJLtAJI8FPhn4AzgccB7gXdOZXJJWkSW9D0AQFV9YvxykpcAN9FF5MrR1e+uqtNG2/8YeDFdIL4AvAy4AnhlVRVwSZI9gDev7/6SrABWANz7QdvN9XIkabDmxZFGkkcm+WiS7yS5Cfgh3Wy7je32tbGvrxr9d6fRf/cCzh8FY53z7un+qur4qlpWVcuW3vfec7ACSVoc5sWRBvApuiOKo4AfAGuAi4Hxp6dWr/uiqioJzJPoSdJi0fuDbpIdgT2Bt1XVmVX1TeA+zC5o3wT2z6gkIwfM4ZiSJOZBNIAbgOuAI5P8QpKnAx+gO9po9QFgd+A9SR6V5LnA/5zzSSVpkes9GlW1Fng+sA9wEfB+4Bhg1Sxu4wrgN4GDgP8H/AHwR3M+rCQtcvPinEZVnQXsPePq8Zc1ZcY2qiozLn8a+PSM3U6ekwElScA8ONKQJC0cRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkprNi88I79PqtVty7W3b9j3GnLv+9m36HmFi7veI6/seYSLqzjv7HkHaKI80JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJajb4aCRZ2vcMkjQUTdFIsjLJB5Icl+SG0Z93JdlitH1pknckuTLJbUn+Pckzx75/eZJKckiS/0xyR5KvJHni2D5HJLklybOTXDra5+wkj5gxy7NH33tHksuSvHU8DEkuT/KmJH+X5Ebg5M39S5IkdWZzpPHC0f5PBo4CVgCvGm37EPB04AXA3sCHgdOS7DvjNo4F/hBYBnwX+FSSbca2bw28EXjJ6H62BP4pSQBGIToZeB/wGOClwHOBt824n1cDl4zu549nsUZJ0gYsmcW+VwOvrKoCLkmyB/DqJKcChwK7V9UVo33fl+RX6eLy8rHbeHNVnQ6Q5CXAlXSh+ZuxeY6uqi+O9jmMLi4HAmcCbwDeVVUfGu3/nSR/CJyU5LWj2QA+X1XvvKeFJFlBFz2W7rT9LP4KJGlxm82RxvljD8oA5wG7Ak8FAlw8enrpliS3AAcDj5xxG+et+6KqbgG+Djx6bPta4IKxfb4HXDW2zxOBN8y4n48C2wI7j93OhRtaSFUdX1XLqmrZkh222dCukqQxsznS2JACngSsnnH97Zt4W/dkC+DPgH9cz7Zrx76+dRPuV5K0EbOJxv5JMna0cQDdUcB5dEcaO1fV2Ru5jQPonm4iybZ05z9OHNu+BbAf8KXRPrsBDwa+Odr+VWDPqvr2LOaWJM2R2UTjwcB7kvwV8FjgtcBbqurSJCcDJyT5X3QP7PcHlgPfrap/GruNP0lyLV1s/hS4k+7ppXXWjO7jaLqjlHcD36A7nwHwv+lOnn8POGW0/97AflX1ulmsRZK0CWYTjZPpXs30ZbqnkP6W7kEdulc7vQF4J/AQ4Hq6cxMzjzz+CPhz4FF0MTikqsafSloFvJXu6GM34HzgN9cd3VTV6UkOBo4BXkMXjUuBE2axDknSJppNNNZU1SuAV8zcUFWrgTeN/mzIl6pqnw3tUFWnAqduYPu/Av+6ge27b2QGSdImGvxvhEuS5o7RkCQ1a3p6qqqWb86dVNVKuldYbWifE/DchCTNax5pSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpo1fXLfkO117xs4b99P9D3GnDvllh36HmFiPnzg0/oeYSLWrK2+R5A2yiMNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJarYoo5FkRZILk1x47Y/v6nscSVowFmU0qur4qlpWVcseuOOWfY8jSQvGooyGJGnTGA1JUjOjIUlqNthoJHlFkkv6nkOShmSw0QAeADyq7yEkaUgGG42qelNVpe85JGlIBhsNSdLcMxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktRsSd8D9O0Ha+7NH/9wn77HmHNXr9qh7xEm5rsv3a3vESbiYf/n2r5HmJi6886+R9Ac8UhDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSp2YKJRpLXJLm87zkkaTFbMNGQJPVvTqKRZPsk952L25rFfT4wyb2meZ+StNhtcjSSbJnkmUk+ClwD7Du6fockxyf5UZKbk3w+ybKx7zsiyS1JDkxyUZJbk5yd5OEzbv91Sa4Z7XsisN2MEZ4FXDO6r6ds6jokSe1mHY0kj0nyTuD7wD8AtwIHAeckCfBpYFfgEODxwDnAWUl2GbuZrYHXAy8FngzcF/jA2H08D3gL8EbgCcB/Aa+eMcrJwAuA+wBnJPl2kj+dGR9J0txpikaSHZO8MslXgP8A9gSOBnauqiOr6pyqKuAZwOOA51bVBVX17ao6BvgucNjYTS4Bfm+0z9eAY4Hlo+gAvAr4cFV9sKouraq3AheMz1RVa6rqM1V1KLAz8LbR/X8rycokL00y8+hk3XpWJLkwyYW33bCq5a9AkkT7kcbvA8cBdwB7VNVzquofq+qOGfs9EdgGuHb0tNItSW4B9gYeObbfqqr6r7HLVwFLgfuNLu8FnDfjtmde/qmquqmq/q6qngE8CXgQ8LfAc+9h/+OrallVLdvmfltvYNmSpHFLGvc7HlgNvBi4KMkngY8A/1ZVd43ttwXwQ+Bp67mNm8a+XjNjW419/6wl2Zru6bAX0Z3r+Abd0cqpm3J7kqT1a3qQrqqrquqtVfUo4FeBW4CPAVcm+fMkjxvt+lW6n/LXjp6aGv/zo1nM9U3ggBnX/czldJ6a5IN0J+LfC3wbeGJVPaGqjquqG2Zxn5KkjZj1T/ZVdX5VvQzYhe5pqz2Af0/yNOBM4IvAqUl+I8nDkzw5yZ+Ntrc6Djg8yZFJfjHJ64H9Z+zzIuBfge2BQ4GHVtVrq+qi2a5JktSm9empn1NVq4CPAx9PshNwV1VVkmfRvfLpr4Gd6J6u+iJw4ixu+x+SPAJ4K905kn8B/gI4Ymy3f6M7EX/Tz9+CJGkSNjka48afeqqqm+leWXX0Pex7AnDCjOtWAplx3duBt8/49jeNbb9q0yeWJG0K30ZEktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUrM5+YzwhezWi8NXHj/Edt7c9wATsxtf6nuEiai+B5AaDPHRUpI0IUZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1MxoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmi3pe4A+JFkBrAC4F9v0PI0kLRyL8kijqo6vqmVVtWwrtu57HElaMBZlNCRJm8ZoSJKaGQ1JUjOjIUlqZjQkSc2MhiSpmdGQJDUzGpKkZkZDktTMaEiSmhkNSVIzoyFJamY0JEnNjIYkqZnRkCQ1MxqSpGZGQ5LUzGhIkpoZDUlSM6MhSWpmNCRJzYyGJKmZ0ZAkNTMakqRmRkOS1CxV1fcMvUpyLfC9Kd3dA4DrpnRf0zbUtbmuhWeoa5vmuh5WVQ9c34ZFH41pSnJhVS3re45JGOraXNfCM9S1zZd1+fSUJKmZ0ZAkNTMa03V83wNM0FDX5roWnqGubV6sy3MakqRmHmlIkpoZDUlSM6MhSWpmNCRJzYyGJKnZ/wdzzCPS1FciVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'salt salt salt salt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "expired-addiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my life . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-664a0a1097ae>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-30-664a0a1097ae>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "amber-latex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ¿ todavia estan en casa ? <end>\n",
      "Predicted translation: are you still at home ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-664a0a1097ae>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-30-664a0a1097ae>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'¿todavia estan en casa?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong translation\n",
    "translate(u'trata de averiguarlo.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-affiliate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
